{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My name is Sandeep of CSE-10, currently living in BBSR.',\n",
       " 'Dr. John has done ph.D at IIT Madras located in Tamil Nadu on 24/01/2023.',\n",
       " 'And yay!',\n",
       " \"we're going to meet soon\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "cor = \"My name is Sandeep of CSE-10, currently living in BBSR. Dr. John has done ph.D at IIT Madras located in Tamil Nadu on 24/01/2023. And yay! we're going to meet soon\"\n",
    "sent = sent_tokenize(cor)\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is Sandeep of CSE-10, currently living in BBSR.\n",
      "Dr. John has done ph.D at IIT Madras located in Tamil Nadu on 24/01/2023.\n",
      "And yay!\n",
      "we're going to meet soon\n"
     ]
    }
   ],
   "source": [
    "for i in sent:\n",
    "    print(i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dr.',\n",
       " 'John',\n",
       " 'has',\n",
       " 'done',\n",
       " 'ph.D',\n",
       " 'at',\n",
       " 'IIT',\n",
       " 'Madras',\n",
       " 'located',\n",
       " 'in',\n",
       " 'Tamil',\n",
       " 'Nadu',\n",
       " 'on',\n",
       " '24/01/2023',\n",
       " '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "token = word_tokenize(sent[1])\n",
    "token"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming using Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dr.\n",
      "john\n",
      "ha\n",
      "done\n",
      "ph.d\n",
      "at\n",
      "iit\n",
      "madra\n",
      "locat\n",
      "in\n",
      "tamil\n",
      "nadu\n",
      "on\n",
      "24/01/2023\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "for i in token:\n",
    "    print(ps.stem(i))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming using Lancaster Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dr.\n",
      "john\n",
      "has\n",
      "don\n",
      "ph.d\n",
      "at\n",
      "iit\n",
      "madra\n",
      "loc\n",
      "in\n",
      "tamil\n",
      "nadu\n",
      "on\n",
      "24/01/2023\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "ls = LancasterStemmer()\n",
    "for i in token:\n",
    "    print(ls.stem(i))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming using Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dr.\n",
      "john\n",
      "has\n",
      "done\n",
      "ph.d\n",
      "at\n",
      "iit\n",
      "madra\n",
      "locat\n",
      "in\n",
      "tamil\n",
      "nadu\n",
      "on\n",
      "24/01/2023\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "ss = SnowballStemmer('english')\n",
    "for i in token:\n",
    "    print(ss.stem(i))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization using WordNet Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.\n",
      "John\n",
      "ha\n",
      "done\n",
      "ph.D\n",
      "at\n",
      "IIT\n",
      "Madras\n",
      "located\n",
      "in\n",
      "Tamil\n",
      "Nadu\n",
      "on\n",
      "24/01/2023\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    "for i in token:\n",
    "    print(wl.lemmatize(i))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dr.', 'NNP'),\n",
       " ('John', 'NNP'),\n",
       " ('has', 'VBZ'),\n",
       " ('done', 'VBN'),\n",
       " ('ph.D', 'NN'),\n",
       " ('at', 'IN'),\n",
       " ('IIT', 'NNP'),\n",
       " ('Madras', 'NNP'),\n",
       " ('located', 'VBN'),\n",
       " ('in', 'IN'),\n",
       " ('Tamil', 'NNP'),\n",
       " ('Nadu', 'NNP'),\n",
       " ('on', 'IN'),\n",
       " ('24/01/2023', 'CD'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "pos_tag(token)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy for lemmatisation and pos tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My my PRON poss\n",
      "name name NOUN nsubj\n",
      "is be AUX ROOT\n",
      "Sandeep Sandeep PROPN attr\n",
      "of of ADP prep\n",
      "CSE-10 CSE-10 PROPN pobj\n",
      ", , PUNCT punct\n",
      "currently currently ADV advmod\n",
      "living live VERB advcl\n",
      "in in ADP prep\n",
      "BBSR BBSR PROPN pobj\n",
      ". . PUNCT punct\n",
      "Dr. Dr. PROPN compound\n",
      "John John PROPN nsubj\n",
      "has have AUX aux\n",
      "done do VERB ROOT\n",
      "ph ph NOUN compound\n",
      ". . PUNCT compound\n",
      "D D PROPN dobj\n",
      "at at ADP prep\n",
      "IIT IIT PROPN compound\n",
      "Madras Madras PROPN pobj\n",
      "located locate VERB acl\n",
      "in in ADP prep\n",
      "Tamil Tamil PROPN compound\n",
      "Nadu Nadu PROPN pobj\n",
      "on on ADP prep\n",
      "24/01/2023 24/01/2023 NUM pobj\n",
      ". . PUNCT punct\n",
      "And and CCONJ cc\n",
      "yay yay INTJ ROOT\n",
      "! ! PUNCT punct\n",
      "we we PRON nsubj\n",
      "'re be AUX aux\n",
      "going go VERB ROOT\n",
      "to to PART aux\n",
      "meet meet VERB xcomp\n",
      "soon soon ADV advmod\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(cor)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.dep_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sandeep GPE\n",
      "John PERSON\n",
      "IIT Madras ORG\n",
      "Tamil Nadu PERSON\n",
      "24/01/2023 DATE\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-gram Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 'my'), ('my', 'name'), ('name', 'is'), ('is', 'Sandeep'), ('Sandeep', 'and'), ('and', 'I'), ('I', 'am'), ('am', 'a'), ('a', 'student'), ('student', 'from'), ('from', 'BBSR'), ('BBSR', '.')]\n",
      "[('Hello', 'my', 'name'), ('my', 'name', 'is'), ('name', 'is', 'Sandeep'), ('is', 'Sandeep', 'and'), ('Sandeep', 'and', 'I'), ('and', 'I', 'am'), ('I', 'am', 'a'), ('am', 'a', 'student'), ('a', 'student', 'from'), ('student', 'from', 'BBSR'), ('from', 'BBSR', '.')]\n",
      "[('Hello', 'my', 'name', 'is'), ('my', 'name', 'is', 'Sandeep'), ('name', 'is', 'Sandeep', 'and'), ('is', 'Sandeep', 'and', 'I'), ('Sandeep', 'and', 'I', 'am'), ('and', 'I', 'am', 'a'), ('I', 'am', 'a', 'student'), ('am', 'a', 'student', 'from'), ('a', 'student', 'from', 'BBSR'), ('student', 'from', 'BBSR', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams, bigrams, trigrams\n",
    "\n",
    "token = nltk.word_tokenize(\"Hello my name is Sandeep and I am a student from BBSR.\")\n",
    "\n",
    "print(list(bigrams(token)))\n",
    "print(list(trigrams(token)))\n",
    "print(list(ngrams(token, 4)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
